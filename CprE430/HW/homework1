I think this is mostly a fact that the designers set up a layered architecture. A layer in the architecture is meant to abstract what is underneath it, and I think the layers are also meant to be interoperable. What I mean is you could swap out one implementation for a layer with another (like I can use Wi-Fi or Ethernet on my laptop), and thus a layer N needs to be able to pass data to any N-1 layer implementation to use an N-1 service. So, setting a fixed size packet for a layer N may not work for every N-1 implementation, since that lower layer may have a max size that in its design or due to a physical/implemented limitation. I think fragmentation is the designers' way of getting optimal performance and keeping complete compatibility. 

The US government / DoD along decided to adopt OSI in the late 80s. However, TCP/IP was already the standard being followed and implemented in Unix, the first being in BSD4.2 in 1983. Unix was widely used even by the government itself. The idea for the government was that TCP/IP would be temporary, and they would start transitioning to OSI supported devices. It seemed with the already widely supported TCP/IP implementation that OSI couldn't get support from vendors/manufacturers to implement it. As well, since many governments and telecom companies supported OSI, it seemed to become politicized. Where it was the governments and telecom monopolies saying to transition to OSI, which was much more complex, versus most people using the internet at the time already using TCP/IP. And the people got their say.    

[1] - https://www.geeksforgeeks.org/critique-of-the-osi-model-and-protocols/
[2] - https://www.rfc-editor.org/rfc/rfc2235.txt
[3] - https://klarasystems.com/articles/history-of-freebsd-part-4-bsd-and-tcp-ip/

